{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作业处理\n",
    "1. 在 100 万例训练集上训练（没有限制）的决策树的近似深度是多少？\n",
    "包含m个叶子的均衡二叉树的深度大概等于$log_2(m)^3$ 。如果训练集包含一百万个实例，则决策树将具有$log_2(10^6)≈20$的深度。\n",
    "\n",
    "2. 节点的基尼指数比起它的父节点是更高还是更低？它是通常情况下更高/更低，还是永远更高/更低？\n",
    "节点的Gini比父节点更低，这是通过CART训练算法的成本函数来确保的，该算法将每个节点分开，以最小化其子节点Gini杂质的加权和。  \n",
    "\n",
    "3. 如果决策树过拟合了，减少最大深度是一个好的方法吗？\n",
    "是的，可以通过约束模型，使模型正则化。\n",
    "\n",
    "4. 如果决策树对训练集欠拟合了，尝试缩放输入特征是否是一个好主意？\n",
    "决策树不关心训练数据是否按比例缩放或居中；对于决策树欠拟合训练集，缩放输入特征作用不大。\n",
    "\n",
    "5. 如果对包含 100 万个实例的数据集训练决策树模型需要一个小时，在包含 1000 万个实例的培训集上训练另一个决策树大概需要多少时间呢？\n",
    "训练决策树的计算复杂度是$O(n × m log(m))$，因此，如果将训练集大小乘以10，则训练时间将乘以$K = (n × 10m × log(10m)) / (n × m × log(m)) = 10 × log(10m) / log(m)$\n",
    "如果m = 10^6，则K≈11.7，因此你可以预期训练时间大约为11.7小时。\n",
    "\n",
    "6. 如果你的训练集包含 100,000 个实例，设置presort=True会加快训练的速度吗？\n",
    "不会，教程中说是小于几千的时候做presort会加快速度。\n",
    "\n",
    "7. 对moons数据集进行决策树训练并优化模型。\n",
    "\n",
    " - 通过语句make_moons(n_samples=10000, noise=0.4)生成moons数据集\n",
    "\n",
    " - 通过train_test_split()将数据集分割为训练集和测试集。\n",
    "\n",
    " - 进行交叉验证，并使用网格搜索法寻找最好的超参数值（使用GridSearchCV类的帮助文档）  \n",
    "**提示: 尝试各种各样的max_leaf_nodes值。使用这些超参数训练全部的训练集数据，并在测试集上测量模型的表现。你应该获得大约 85% 到 87% 的准确度。**\n",
    "~~~python\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "#获取数据\n",
    "X,y=make_moons(n_samples=10000, noise=0.4)\n",
    "#分割数据\n",
    "X_train,X_test, y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "使用网格搜索进行交叉验证，找出好的超参数，注意里面params参数设定\n",
    "params={\n",
    "    \"max_leaf_nodes\":list(range(2,100)),\n",
    "    \"min_samples_split\":[2,3,4]\n",
    "}\n",
    "grid_search_cv=GridSearchCV(DecisionTreeClassifier(random_state=42),params,n_jobs=-1,verbose=1,cv=3)\n",
    "grid_search_cv.fit(X_train,y_train)\n",
    "#训练结果\n",
    "'''\n",
    "Fitting 3 folds for each of 294 candidates, totalling 882 fits\n",
    "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   14.5s\n",
    "[Parallel(n_jobs=-1)]: Done 882 out of 882 | elapsed:   16.3s finished\n",
    "GridSearchCV(cv=3, error_score='raise',\n",
    "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "            max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
    "            splitter='best'),\n",
    "       fit_params=None, iid=True, n_jobs=-1,\n",
    "       param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], 'min_samples_split': [2, 3, 4]},\n",
    "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "       scoring=None, verbose=1)\n",
    "'''\n",
    "#正确率86%\n",
    "y_pred=grid_search_cv.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)\n",
    "~~~\n",
    "\n",
    "8. 生成森林\n",
    " - 接着前边的练习，现在，让我们生成 1,000 个训练集的子集，每个子集包含 100 个随机选择的实例。提示：你可以使用 Scikit-Learn 的ShuffleSplit类。\n",
    " - 使用上面找到的最佳超参数值，在每个子集上训练一个决策树。在测试集上测试这 1000 个决策树。由于它们是在较小的集合上进行了训练，因此这些决策树可能会比第一个决策树效果更差，只能达到约 80% 的准确度。\n",
    " - 见证奇迹的时刻到了！对于每个测试集实例，生成 1,000 个决策树的预测结果，然后只保留出现次数最多的预测结果（您可以使用 SciPy 的mode()函数）。这个函数使你可以对测试集进行多数投票预测。\n",
    " - 在测试集上评估这些预测结果，你应该获得了一个比第一个模型高一点的准确率，（大约 0.5% 到 1.5%），恭喜，你已经弄出了一个随机森林分类器模型!\n",
    "~~~python\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "#生成 1,000 个训练集的子集，每个子集包含 100 个随机选择的实例。提示：你可以使用 Scikit-Learn 的ShuffleSplit类。\n",
    "n_tree=1000\n",
    "n_instance=1000\n",
    "mini_sets=[]\n",
    "rs=ShuffleSplit(n_splits=n_tree,test_size=len(X_train)-n_instance,random_state=42)\n",
    "for mini_train_index ,mini_test_index in rs.split(X_train):\n",
    "    X_mini_train = X_train[mini_train_index]\n",
    "    y_mini_train = y_train[mini_train_index]\n",
    "    mini_sets.append((X_mini_train, y_mini_train))\n",
    "    \n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "#使用上面找到的最佳超参数值，在每个子集上训练一个决策树。在测试集上测试这 1000 个决策树。由于它们是在较小的集合上进行了训练，因此这些决策树可能会比第一个决策树效果更差，只能达到约 80% 的准确度。\n",
    "forest=[clone(grid_search_cv.best_estimator_)for _ in range(n_tree)]\n",
    "accuracy_scores=[]\n",
    "for tree, (X_mini_train,y_mini_train) in zip(forest,mini_sets):\n",
    "    tree.fit(X_mini_train,y_mini_train)\n",
    "    y_pred=tree.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test,y_pred))\n",
    "np.mean(accuracy_scores)\n",
    "\n",
    "# 见证奇迹的时刻到了！对于每个测试集实例，生成 1,000 个决策树的预测结果，然后只保留出现次数最多的预测结果（您可以使用 SciPy 的mode()函数）。这个函数使你可以对测试集进行多数投票预测。\n",
    "Y_pred=np.empty([n_tree,len(X_test)],dtype=np.uint8)\n",
    "for tree_index, tree in enumerate(forest):\n",
    "    Y_pred[tree_index]=tree.predict(X_test)\n",
    "\n",
    "from scipy.stats import mode\n",
    "y_pred_majority_votes, n_votes = mode(Y_pred, axis=0)\n",
    "\n",
    "#在测试集上评估这些预测结果，你应该获得了一个比第一个模型高一点的准确率，（大约 0.5% 到 1.5%），恭喜，你已经弄出了一个随机森林分类器模型!\n",
    "accuracy_score(y_test, y_pred_majority_votes.reshape([-1]))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 294 candidates, totalling 882 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=-1)]: Done 882 out of 882 | elapsed:   16.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99], 'min_samples_split': [2, 3, 4]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "X,y=make_moons(n_samples=10000, noise=0.4)\n",
    "X_train,X_test, y_train,y_test=train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "params={\n",
    "    \"max_leaf_nodes\":list(range(2,100)),\n",
    "    \"min_samples_split\":[2,3,4]\n",
    "}\n",
    "grid_search_cv=GridSearchCV(DecisionTreeClassifier(random_state=42),params,n_jobs=-1,verbose=1,cv=3)\n",
    "grid_search_cv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.856"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=grid_search_cv.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 生成森林\n",
    " - 接着前边的练习，现在，让我们生成 1,000 个训练集的子集，每个子集包含 100 个随机选择的实例。提示：你可以使用 Scikit-Learn 的ShuffleSplit类。\n",
    " - 使用上面找到的最佳超参数值，在每个子集上训练一个决策树。在测试集上测试这 1000 个决策树。由于它们是在较小的集合上进行了训练，因此这些决策树可能会比第一个决策树效果更差，只能达到约 80% 的准确度。\n",
    " - 见证奇迹的时刻到了！对于每个测试集实例，生成 1,000 个决策树的预测结果，然后只保留出现次数最多的预测结果（您可以使用 SciPy 的mode()函数）。这个函数使你可以对测试集进行多数投票预测。\n",
    " - 在测试集上评估这些预测结果，你应该获得了一个比第一个模型高一点的准确率，（大约 0.5% 到 1.5%），恭喜，你已经弄出了一个随机森林分类器模型!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "#生成 1,000 个训练集的子集，每个子集包含 100 个随机选择的实例。提示：你可以使用 Scikit-Learn 的ShuffleSplit类。\n",
    "n_tree=1000\n",
    "n_instance=1000\n",
    "mini_sets=[]\n",
    "rs=ShuffleSplit(n_splits=n_tree,test_size=len(X_train)-n_instance,random_state=42)\n",
    "for mini_train_index ,mini_test_index in rs.split(X_train):\n",
    "    X_mini_train = X_train[mini_train_index]\n",
    "    y_mini_train = y_train[mini_train_index]\n",
    "    mini_sets.append((X_mini_train, y_mini_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8450045"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "#使用上面找到的最佳超参数值，在每个子集上训练一个决策树。在测试集上测试这 1000 个决策树。由于它们是在较小的集合上进行了训练，因此这些决策树可能会比第一个决策树效果更差，只能达到约 80% 的准确度。\n",
    "forest=[clone(grid_search_cv.best_estimator_)for _ in range(n_tree)]\n",
    "accuracy_scores=[]\n",
    "for tree, (X_mini_train,y_mini_train) in zip(forest,mini_sets):\n",
    "    tree.fit(X_mini_train,y_mini_train)\n",
    "    y_pred=tree.predict(X_test)\n",
    "    accuracy_scores.append(accuracy_score(y_test,y_pred))\n",
    "np.mean(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 见证奇迹的时刻到了！对于每个测试集实例，生成 1,000 个决策树的预测结果，然后只保留出现次数最多的预测结果（您可以使用 SciPy 的mode()函数）。这个函数使你可以对测试集进行多数投票预测。\n",
    "Y_pred=np.empty([n_tree,len(X_test)],dtype=np.uint8)\n",
    "for tree_index, tree in enumerate(forest):\n",
    "    Y_pred[tree_index]=tree.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "y_pred_majority_votes, n_votes = mode(Y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.858"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在测试集上评估这些预测结果，你应该获得了一个比第一个模型高一点的准确率，（大约 0.5% 到 1.5%），恭喜，你已经弄出了一个随机森林分类器模型!\n",
    "accuracy_score(y_test, y_pred_majority_votes.reshape([-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
